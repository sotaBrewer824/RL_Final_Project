# ğŸ¯ Kalman-Based Reward Normalization in Policy Gradient RL

This repository contains our course project on **accelerating policy gradient reinforcement learning** using Kalman-based reward normalization. We propose replacing conventional mean-std normalization with a lightweight 1D Kalman filter to stabilize training and improve sample efficiency.

ğŸ“ All notebooks are designed to be run directly in **Google Colab**, requiring no local setup.

---

## ğŸ“‚ Project Structure

```
ğŸ“ Root
â”œâ”€â”€ ğŸ““ Actor Critic [CartPole] Main.ipynb
â”œâ”€â”€ ğŸ““ Proximal Policy Optimization.ipynb
â”œâ”€â”€ ğŸ““ Generalized Advantage Estimation.ipynb
â”œâ”€â”€ ğŸ““ [LunarLander]_Original.ipynb
â”œâ”€â”€ ğŸ““ [LunarLander]_KF.ipynb
â”œâ”€â”€ ğŸ“ Visualizations/
â”œâ”€â”€ ğŸ“„ Final_Report.pdf
â”œâ”€â”€ ğŸ“„ Presentation.pdf
```

---

## âš™ï¸ How to Run

Open any notebook in **Google Colab**, for example:

- [Actor Critic [CartPole] Main.ipynb](https://colab.research.google.com/)  
- [Proximal Policy Optimization.ipynb](https://colab.research.google.com/)  
- [Generalized Advantage Estimation.ipynb](https://colab.research.google.com/)

> âš ï¸ No setup needed â€” all experiments are cloud-ready and use standard Python RL libraries.

---

## ğŸ“˜ Notebooks Overview

| Notebook | Description |
|----------|-------------|
| `Actor Critic [CartPole] Main.ipynb` | PPO/AC with Kalman and mean-std reward normalization on CartPole |
| `Proximal Policy Optimization.ipynb` | PPO with ablation on normalization methods (Kalman/simple/mean-std) |
| `Generalized Advantage Estimation.ipynb` | GAE-based training under different normalization |
| `[LunarLander]_Original.ipynb` | Baseline training on LunarLander with standard normalization |
| `[LunarLander]_KF.ipynb` | Kalman-based reward normalization applied to LunarLander |

---

## ğŸ“Š Visualizations

All training plots and result figures can be found in the [`Visualizations/`](./Visualizations) folder.

Example figure:
<p align="center">
  <img src="Visualizations/ppo_cartpole_compare.png" width="600"/>
</p>

---

## ğŸ“„ Additional Materials

- ğŸ“„ `Final_Report.pdf` â€“ Full mid-term project report with method, experiments, and analysis  
- ğŸï¸ `Presentation.pdf` â€“ Slides used for project presentation

---

## ğŸ§­ Summary

Kalman-based reward normalization provides:

- âœ”ï¸ Faster convergence in early-stage training  
- âœ”ï¸ Reduced variance in noisy environments  
- âœ”ï¸ Strong compatibility with existing RL pipelines (AC, GAE, PPO)

---

## ğŸ”® Future Work

- End-to-end learnable Kalman parameters $(Q, R)$  
- Multi-dimensional or state-dependent filtering  
- Extensions to continuous control, offline RL, and safe-RL

---

## ğŸ“œ License

MIT License â€“ feel free to use and extend!
